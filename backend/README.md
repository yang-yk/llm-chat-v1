# LLM Chat System - åç«¯æœåŠ¡

åŸºäº FastAPI çš„å¤§æ¨¡å‹å¯¹è¯åç«¯æœåŠ¡ï¼Œæä¾›å®Œæ•´çš„å¯¹è¯ç®¡ç†å’ŒLLM APIé›†æˆã€‚

## æŠ€æœ¯æ ˆ

- **Webæ¡†æ¶**: FastAPI
- **æ•°æ®åº“**: SQLite (å¯åˆ‡æ¢åˆ° PostgreSQL/MySQL)
- **ORM**: SQLAlchemy
- **HTTPå®¢æˆ·ç«¯**: httpx (å¼‚æ­¥)
- **ASGIæœåŠ¡å™¨**: Uvicorn

## åŠŸèƒ½ç‰¹æ€§

- âœ¨ **å¤šè½®å¯¹è¯**: æ”¯æŒä¸Šä¸‹æ–‡è¿è´¯çš„å¤šè½®å¯¹è¯
- ğŸ’¾ **å¯¹è¯å­˜å‚¨**: è‡ªåŠ¨ä¿å­˜æ‰€æœ‰å¯¹è¯è®°å½•
- ğŸ”„ **ä¼šè¯ç®¡ç†**: åˆ›å»ºã€æŸ¥è¯¢ã€åˆ é™¤å¯¹è¯ä¼šè¯
- ğŸ“ **å†å²è®°å½•**: è·å–å®Œæ•´å¯¹è¯å†å²
- ğŸš€ **å¼‚æ­¥å¤„ç†**: é«˜æ€§èƒ½å¼‚æ­¥æ¶æ„
- ğŸ“¡ **æµå¼å“åº”**: æ”¯æŒ Server-Sent Events (SSE)
- âš™ï¸ **æ¨¡å‹é…ç½®**: æ”¯æŒå¤šç§é¢„è®¾æ¨¡å‹å’Œè‡ªå®šä¹‰æ¨¡å‹
- ğŸ‘¥ **ç”¨æˆ·é…ç½®**: æ¯ä¸ªç”¨æˆ·ç‹¬ç«‹çš„æ¨¡å‹é…ç½®
- ğŸ·ï¸ **è‡ªåŠ¨æ ‡é¢˜**: æ ¹æ®å¯¹è¯å†…å®¹è‡ªåŠ¨ç”Ÿæˆæ ‡é¢˜

## é¡¹ç›®ç»“æ„

```
backend/
â”œâ”€â”€ main.py                    # FastAPIä¸»åº”ç”¨å’Œè·¯ç”±
â”œâ”€â”€ config.py                  # é…ç½®æ–‡ä»¶
â”œâ”€â”€ database.py                # æ•°æ®åº“æ¨¡å‹å’Œä¼šè¯ç®¡ç†
â”œâ”€â”€ llm_service.py             # å¤§æ¨¡å‹APIè°ƒç”¨æœåŠ¡
â”œâ”€â”€ conversation_service.py    # å¯¹è¯ç®¡ç†æœåŠ¡
â””â”€â”€ requirements.txt           # Pythonä¾èµ–
```

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. é…ç½®ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰

åˆ›å»º `.env` æ–‡ä»¶æˆ–ä½¿ç”¨é»˜è®¤é…ç½®ï¼š

```bash
# å¤§æ¨¡å‹APIé…ç½®
LLM_API_URL=http://111.19.168.151:11551/v1/chat/completions
LLM_MODEL=codegeex4-all-9b
LLM_API_KEY=codegeex

# æ•°æ®åº“é…ç½®
DATABASE_URL=sqlite:///./conversation.db

# æœåŠ¡å™¨é…ç½®
HOST=0.0.0.0
PORT=8000
```

### 3. å¯åŠ¨æœåŠ¡

```bash
python main.py
```

æˆ–ä½¿ç”¨ uvicornï¼š

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### 4. è®¿é—®æœåŠ¡

- **APIæ–‡æ¡£**: http://localhost:8000/docs
- **å¥åº·æ£€æŸ¥**: http://localhost:8000/api/health

## APIæ¥å£

### å¯¹è¯ç®¡ç†

#### åˆ›å»ºå¯¹è¯
```http
POST /conversations
```

**å“åº”**:
```json
{
  "session_id": "uuid",
  "message": "å¯¹è¯ä¼šè¯åˆ›å»ºæˆåŠŸ"
}
```

#### è·å–å¯¹è¯åˆ—è¡¨
```http
GET /conversations
```

**å“åº”**:
```json
{
  "conversations": [
    {
      "session_id": "uuid",
      "title": "å¯¹è¯æ ‡é¢˜",
      "created_at": "2025-10-02T10:00:00",
      "updated_at": "2025-10-02T10:30:00",
      "message_count": 10
    }
  ]
}
```

#### è·å–å¯¹è¯å†å²
```http
GET /conversations/{session_id}/history
```

**å“åº”**:
```json
{
  "session_id": "uuid",
  "messages": [
    {
      "role": "user",
      "content": "ä½ å¥½"
    },
    {
      "role": "assistant",
      "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
    }
  ]
}
```

#### åˆ é™¤å¯¹è¯
```http
DELETE /conversations/{session_id}
```

### æ¶ˆæ¯å‘é€

#### å‘é€æ¶ˆæ¯ï¼ˆæ”¯æŒæµå¼/éæµå¼ï¼‰
```http
POST /chat
```

**è¯·æ±‚ä½“**:
```json
{
  "user_id": "user_xxx",
  "session_id": "uuid",
  "message": "ä½ å¥½",
  "temperature": 0.7,
  "max_tokens": 2000,
  "stream": true
}
```

**éæµå¼å“åº”** (`stream: false`):
```json
{
  "session_id": "uuid",
  "user_message": "ä½ å¥½",
  "assistant_reply": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
}
```

**æµå¼å“åº”** (`stream: true`):
```
data: {"chunk": "ä½ å¥½"}
data: {"chunk": "ï¼"}
data: {"chunk": "æœ‰ä»€ä¹ˆ"}
data: {"chunk": "å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}
data: {"done": true}
```

### æ¨¡å‹é…ç½®

#### è·å–é…ç½®
```http
GET /api/config?user_id=user_xxx
```

**å“åº”**:
```json
{
  "llm_api_url": "http://...",
  "llm_model": "codegeex4-all-9b",
  "llm_api_key": "xxx",
  "preset_models": [...],
  "current_model_type": "codegeex",
  "max_tokens": 2000
}
```

#### æ›´æ–°é…ç½®
```http
POST /api/config
```

**è¯·æ±‚ä½“**:
```json
{
  "user_id": "user_xxx",
  "model_type": "codegeex",
  "max_tokens": 2000
}
```

## æ•°æ®åº“

### æ•°æ®è¡¨ç»“æ„

#### conversations è¡¨
- `id` - ä¸»é”®
- `session_id` - ä¼šè¯å”¯ä¸€æ ‡è¯†ç¬¦
- `title` - å¯¹è¯æ ‡é¢˜
- `created_at` - åˆ›å»ºæ—¶é—´
- `updated_at` - æ›´æ–°æ—¶é—´

#### messages è¡¨
- `id` - ä¸»é”®
- `conversation_id` - å¤–é”®
- `role` - è§’è‰²ï¼ˆuser/assistantï¼‰
- `content` - æ¶ˆæ¯å†…å®¹
- `created_at` - åˆ›å»ºæ—¶é—´

#### user_configs è¡¨
- `id` - ä¸»é”®
- `user_identifier` - ç”¨æˆ·æ ‡è¯†
- `current_model_type` - å½“å‰æ¨¡å‹ç±»å‹
- `max_tokens` - æœ€å¤§tokenæ•°
- `custom_api_url` - è‡ªå®šä¹‰API URL
- `custom_model` - è‡ªå®šä¹‰æ¨¡å‹å
- `custom_api_key` - è‡ªå®šä¹‰APIå¯†é’¥

### åˆ‡æ¢æ•°æ®åº“

ä¿®æ”¹ `config.py` ä¸­çš„ `DATABASE_URL`ï¼š

```python
# PostgreSQL
DATABASE_URL = "postgresql://user:password@localhost/dbname"

# MySQL
DATABASE_URL = "mysql+pymysql://user:password@localhost/dbname"
```

## é¢„è®¾æ¨¡å‹

ç³»ç»Ÿå†…ç½®ä¸¤ä¸ªé¢„è®¾æ¨¡å‹ï¼š

1. **CodeGeex 4**
   - URL: http://111.19.168.151:11551/v1/chat/completions
   - Model: codegeex4-all-9b
   - Key: codegeex

2. **GLM-4 32B**
   - URL: http://111.19.168.151:11553/v1/chat/completions
   - Model: glm4_32B_chat
   - Key: glm432b

## å¼€å‘è¯´æ˜

### æ·»åŠ æ–°çš„APIç«¯ç‚¹

åœ¨ `main.py` ä¸­æ·»åŠ æ–°çš„è·¯ç”±ï¼š

```python
@app.get("/api/new-endpoint")
async def new_endpoint():
    return {"message": "æ–°ç«¯ç‚¹"}
```

### ä¿®æ”¹LLMæœåŠ¡

ç¼–è¾‘ `llm_service.py` è‡ªå®šä¹‰LLMè°ƒç”¨é€»è¾‘ï¼š

```python
class LLMService:
    async def chat_completion(self, messages, temperature, max_tokens):
        # è‡ªå®šä¹‰å®ç°
        pass
```

### æ‰©å±•æ•°æ®åº“æ¨¡å‹

åœ¨ `database.py` ä¸­æ·»åŠ æ–°çš„æ¨¡å‹ï¼š

```python
class NewModel(Base):
    __tablename__ = "new_table"
    # å®šä¹‰å­—æ®µ
```

## æ€§èƒ½ä¼˜åŒ–

- ä½¿ç”¨å¼‚æ­¥å¤„ç†æé«˜å¹¶å‘æ€§èƒ½
- SQLiteé€‚åˆä¸­å°è§„æ¨¡ï¼Œå¤§è§„æ¨¡å»ºè®®PostgreSQL
- æµå¼å“åº”å‡å°‘é¦–å­—èŠ‚æ—¶é—´
- è‡ªåŠ¨æ¸…ç†æ—§å¯¹è¯ï¼ˆä¿ç•™æœ€è¿‘500æ¡ï¼‰

## å®‰å…¨å»ºè®®

- ç”Ÿäº§ç¯å¢ƒé…ç½®å…·ä½“çš„CORSåŸŸå
- ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨æ•æ„Ÿä¿¡æ¯
- å®šæœŸå¤‡ä»½æ•°æ®åº“
- æ·»åŠ ç”¨æˆ·è®¤è¯å’Œæˆæƒ
- é™åˆ¶APIè°ƒç”¨é¢‘ç‡

## æ•…éšœæ’æŸ¥

### æ•°æ®åº“è¿æ¥å¤±è´¥
- æ£€æŸ¥ `DATABASE_URL` é…ç½®
- ç¡®ä¿æ•°æ®åº“æ–‡ä»¶æœ‰å†™æƒé™

### LLM APIè°ƒç”¨å¤±è´¥
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- éªŒè¯API URLå’Œå¯†é’¥
- æŸ¥çœ‹æœåŠ¡å™¨æ—¥å¿—

### ç«¯å£è¢«å ç”¨
- ä¿®æ”¹ `config.py` ä¸­çš„ `PORT`
- æˆ–ä½¿ç”¨ `uvicorn main:app --port 8001`

## æ—¥å¿—

æœåŠ¡å™¨è¿è¡Œæ—¥å¿—ä¼šè¾“å‡ºåˆ°æ§åˆ¶å°ï¼ŒåŒ…æ‹¬ï¼š
- è¯·æ±‚ä¿¡æ¯
- é”™è¯¯ä¿¡æ¯
- æ•°æ®åº“æ“ä½œ
- LLM APIè°ƒç”¨çŠ¶æ€

## éƒ¨ç½²

### å¼€å‘ç¯å¢ƒ
```bash
python main.py
```

### ç”Ÿäº§ç¯å¢ƒ
```bash
# ä½¿ç”¨gunicorn + uvicorn worker
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000

# æˆ–ä½¿ç”¨systemdæœåŠ¡
sudo systemctl start llm-backend
```

### Docker
```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## ä¾èµ–è¯´æ˜

ä¸»è¦ä¾èµ–åŒ…ï¼š
- `fastapi` - Webæ¡†æ¶
- `uvicorn` - ASGIæœåŠ¡å™¨
- `sqlalchemy` - ORM
- `httpx` - HTTPå®¢æˆ·ç«¯
- `pydantic` - æ•°æ®éªŒè¯

## è®¸å¯è¯

MIT License
